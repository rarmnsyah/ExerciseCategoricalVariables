{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setup\n\nThe questions below will give you feedback on your work. Run the following cell to set up the feedback system.","metadata":{}},{"cell_type":"code","source":"# Set up code checking\nimport os\nif not os.path.exists(\"../input/train.csv\"):\n    os.symlink(\"../input/home-data-for-ml-course/train.csv\", \"../input/train.csv\")  \n    os.symlink(\"../input/home-data-for-ml-course/test.csv\", \"../input/test.csv\") \n    os.symlink(\"../input/home-data-for-ml-course/sample_submission.csv\", \"../input/sample_submission.csv\")\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.ml_intermediate.ex3 import *\nprint(\"Setup Complete\")","metadata":{"execution":{"iopub.status.busy":"2022-04-07T01:51:32.223815Z","iopub.execute_input":"2022-04-07T01:51:32.224154Z","iopub.status.idle":"2022-04-07T01:51:32.233158Z","shell.execute_reply.started":"2022-04-07T01:51:32.224117Z","shell.execute_reply":"2022-04-07T01:51:32.232160Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"In this exercise, we will work with data from the [Housing Prices Competition for Kaggle Learn Users](https://www.kaggle.com/c/home-data-for-ml-course). \n\n![Ames Housing dataset image](https://i.imgur.com/lTJVG4e.png)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Read the data\nX = pd.read_csv('../input/train.csv', index_col='Id') \nX_test = pd.read_csv('../input/test.csv', index_col='Id')\n\n# Remove rows with missing target, separate target from predictors\nX.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = X.SalePrice\nX.drop(['SalePrice'], axis=1, inplace=True)\n\n# To keep things simple, we'll drop columns with missing values\ncols_with_missing = [col for col in X.columns if X[col].isnull().any()] \nX.drop(cols_with_missing, axis=1, inplace=True)\nX_test.drop(cols_with_missing, axis=1, inplace=True)\n\n# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(X, y,train_size=0.8, test_size=0.2,random_state=0)","metadata":{"execution":{"iopub.status.busy":"2022-04-07T01:49:08.271211Z","iopub.execute_input":"2022-04-07T01:49:08.271544Z","iopub.status.idle":"2022-04-07T01:49:09.598348Z","shell.execute_reply.started":"2022-04-07T01:49:08.271506Z","shell.execute_reply":"2022-04-07T01:49:09.597173Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"Use the next code cell to print the first five rows of the data.","metadata":{}},{"cell_type":"code","source":"X_train.head","metadata":{"execution":{"iopub.status.busy":"2022-04-07T01:49:09.600938Z","iopub.execute_input":"2022-04-07T01:49:09.601449Z","iopub.status.idle":"2022-04-07T01:49:09.628929Z","shell.execute_reply.started":"2022-04-07T01:49:09.601362Z","shell.execute_reply":"2022-04-07T01:49:09.628172Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Notice that the dataset contains both numerical and categorical variables.  We'll need to encode the categorical data before training a model.\n\nTo compare different models, we'll use the same `score_dataset()` function from the tutorial.  This function reports the [mean absolute error](https://en.wikipedia.org/wiki/Mean_absolute_error) (MAE) from a random forest model.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\n# function for comparing different approaches\ndef score_dataset(X_train, X_valid, y_train, y_valid):\n    model = RandomForestRegressor(n_estimators=100, random_state=0)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_valid)\n    return mean_absolute_error(y_valid, preds)","metadata":{"execution":{"iopub.status.busy":"2022-04-07T01:49:09.630339Z","iopub.execute_input":"2022-04-07T01:49:09.630594Z","iopub.status.idle":"2022-04-07T01:49:09.870652Z","shell.execute_reply.started":"2022-04-07T01:49:09.630564Z","shell.execute_reply":"2022-04-07T01:49:09.869641Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Step 1: Drop columns with categorical data\n\nWe'll get started with the most straightforward approach.  Use the code cell below to preprocess the data in `X_train` and `X_valid` to remove columns with categorical data.  Set the preprocessed DataFrames to `drop_X_train` and `drop_X_valid`, respectively.  ","metadata":{}},{"cell_type":"code","source":"# Fill in the lines below: drop columns in training and validation data\ndrop_X_train = X_train.select_dtypes(exclude = ['object'])\ndrop_X_valid = X_valid.select_dtypes(exclude = ['object'])","metadata":{"execution":{"iopub.status.busy":"2022-04-07T01:49:09.875626Z","iopub.execute_input":"2022-04-07T01:49:09.875930Z","iopub.status.idle":"2022-04-07T01:49:09.886849Z","shell.execute_reply.started":"2022-04-07T01:49:09.875897Z","shell.execute_reply":"2022-04-07T01:49:09.885780Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Run the next code cell to get the MAE for this approach.","metadata":{}},{"cell_type":"code","source":"print(\"MAE from Approach 1 (Drop categorical variables):\")\nprint(score_dataset(drop_X_train, drop_X_valid, y_train, y_valid))","metadata":{"execution":{"iopub.status.busy":"2022-04-07T01:49:09.888916Z","iopub.execute_input":"2022-04-07T01:49:09.889317Z","iopub.status.idle":"2022-04-07T01:49:10.989674Z","shell.execute_reply.started":"2022-04-07T01:49:09.889270Z","shell.execute_reply":"2022-04-07T01:49:10.988760Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Step 2: Ordinal encoding\n\n### Part A\n\nIf you now write code to: \n- fit an ordinal encoder to the training data, and then \n- use it to transform both the training and validation data, \n\nyou'll get an error.  Can you see why this is the case?  (_You'll need  to use the above output to answer this question._)","metadata":{}},{"cell_type":"markdown","source":"Run the code cell below to save the problematic columns to a Python list `bad_label_cols`.  Likewise, columns that can be safely ordinal encoded are stored in `good_label_cols`.","metadata":{}},{"cell_type":"code","source":"# Categorical columns in the training data\nobject_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n\n# Columns that can be safely ordinal encoded\ngood_label_cols = [col for col in object_cols if \n                   set(X_valid[col]).issubset(set(X_train[col]))]\n        \n# Problematic columns that will be dropped from the dataset\nbad_label_cols = list(set(object_cols)-set(good_label_cols))\n        \nprint('Categorical columns that will be ordinal encoded:', good_label_cols)\nprint('\\nCategorical columns that will be dropped from the dataset:', bad_label_cols)","metadata":{"execution":{"iopub.status.busy":"2022-04-07T01:49:10.991067Z","iopub.execute_input":"2022-04-07T01:49:10.992058Z","iopub.status.idle":"2022-04-07T01:49:11.007797Z","shell.execute_reply.started":"2022-04-07T01:49:10.992008Z","shell.execute_reply":"2022-04-07T01:49:11.006778Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Part B\n\nUse the next code cell to ordinal encode the data in `X_train` and `X_valid`.  Set the preprocessed DataFrames to `label_X_train` and `label_X_valid`, respectively.  \n- We have provided code below to drop the categorical columns in `bad_label_cols` from the dataset. \n- You should ordinal encode the categorical columns in `good_label_cols`.  ","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OrdinalEncoder\n\n# Drop categorical columns that will not be encoded\nlabel_X_train = X_train.drop(bad_label_cols, axis=1)\nlabel_X_valid = X_valid.drop(bad_label_cols, axis=1)\n\n# Apply ordinal encoder \nmyEncoder = OrdinalEncoder()\n\nlabel_X_train[good_label_cols] = myEncoder.fit_transform(X_train[good_label_cols])\nlabel_X_valid[good_label_cols] = myEncoder.transform(X_valid[good_label_cols])\n","metadata":{"execution":{"iopub.status.busy":"2022-04-07T01:49:11.010283Z","iopub.execute_input":"2022-04-07T01:49:11.010873Z","iopub.status.idle":"2022-04-07T01:49:11.054245Z","shell.execute_reply.started":"2022-04-07T01:49:11.010799Z","shell.execute_reply":"2022-04-07T01:49:11.053193Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Run the next code cell to get the MAE for this approach.","metadata":{}},{"cell_type":"code","source":"print(\"MAE from Approach 2 (Ordinal Encoding):\") \nprint(score_dataset(label_X_train, label_X_valid, y_train, y_valid))","metadata":{"execution":{"iopub.status.busy":"2022-04-07T01:49:11.056002Z","iopub.execute_input":"2022-04-07T01:49:11.056708Z","iopub.status.idle":"2022-04-07T01:49:12.464860Z","shell.execute_reply.started":"2022-04-07T01:49:11.056652Z","shell.execute_reply":"2022-04-07T01:49:12.463823Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"So far, you've tried two different approaches to dealing with categorical variables.  And, you've seen that encoding categorical data yields better results than removing columns from the dataset.\n\nSoon, you'll try one-hot encoding.  Before then, there's one additional topic we need to cover.  Begin by running the next code cell without changes.  ","metadata":{}},{"cell_type":"code","source":"# Get number of unique entries in each column with categorical data\nobject_nunique = list(map(lambda col: X_train[col].nunique(), object_cols))\nd = dict(zip(object_cols, object_nunique))\n\n# Print number of unique entries by column, in ascending order\nsorted(d.items(), key=lambda x: x[1])\n","metadata":{"execution":{"iopub.status.busy":"2022-04-07T01:49:12.466063Z","iopub.execute_input":"2022-04-07T01:49:12.466325Z","iopub.status.idle":"2022-04-07T01:49:12.480875Z","shell.execute_reply.started":"2022-04-07T01:49:12.466294Z","shell.execute_reply":"2022-04-07T01:49:12.479932Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Step 3: Investigating cardinality\n\n### Part A\n\nThe output above shows, for each column with categorical data, the number of unique values in the column.  For instance, the `'Street'` column in the training data has two unique values: `'Grvl'` and `'Pave'`, corresponding to a gravel road and a paved road, respectively.\n\nWe refer to the number of unique entries of a categorical variable as the **cardinality** of that categorical variable.  For instance, the `'Street'` variable has cardinality 2.\n\nUse the output above to answer the questions below.","metadata":{}},{"cell_type":"code","source":"# How much cardinality in this dataset with more than 10 missing values?\nhigh_cardinality_numcols = len([item for item in d if d[item] > 10])","metadata":{"execution":{"iopub.status.busy":"2022-04-07T01:49:12.483618Z","iopub.execute_input":"2022-04-07T01:49:12.484344Z","iopub.status.idle":"2022-04-07T01:49:12.489735Z","shell.execute_reply.started":"2022-04-07T01:49:12.484286Z","shell.execute_reply":"2022-04-07T01:49:12.488772Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### Part B\n\nFor large datasets with many rows, one-hot encoding can greatly expand the size of the dataset.  For this reason, we typically will only one-hot encode columns with relatively low cardinality.  Then, high cardinality columns can either be dropped from the dataset, or we can use ordinal encoding.\n\nAs an example, consider a dataset with 10,000 rows, and containing one categorical column with 100 unique entries.  \n- If this column is replaced with the corresponding one-hot encoding, how many entries are added to the dataset?  \n- If we instead replace the column with the ordinal encoding, how many entries are added?  \n\nUse your answers to fill in the lines below.","metadata":{}},{"cell_type":"code","source":"# Using one-hot encoding to a huge amount of data would make a lot of data too\n# TOO MUCH MEMORY USED\nOH_entries_added = 1e4*1e2 - 1e4\n\n# So, in order to save memory efficiently we must drop the data\nlabel_entries_added = 0","metadata":{"execution":{"iopub.status.busy":"2022-04-07T01:49:12.491422Z","iopub.execute_input":"2022-04-07T01:49:12.491677Z","iopub.status.idle":"2022-04-07T01:49:12.502016Z","shell.execute_reply.started":"2022-04-07T01:49:12.491648Z","shell.execute_reply":"2022-04-07T01:49:12.501157Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"Next, we'll experiment with one-hot encoding.  But, instead of encoding all of the categorical variables in the dataset, we'll only create a one-hot encoding for columns with cardinality less than 10.\n\nRun the code cell below without changes to set `low_cardinality_cols` to a Python list containing the columns that will be one-hot encoded.  Likewise, `high_cardinality_cols` contains a list of categorical columns that will be dropped from the dataset.","metadata":{}},{"cell_type":"code","source":"# Columns that will be one-hot encoded\nlow_cardinality_cols = [col for col in object_cols if X_train[col].nunique() < 10]\n\n# Columns that will be dropped from the dataset\nhigh_cardinality_cols = list(set(object_cols)-set(low_cardinality_cols))\n\nprint('Categorical columns that will be one-hot encoded:', low_cardinality_cols)\nprint('\\nCategorical columns that will be dropped from the dataset:', high_cardinality_cols)","metadata":{"execution":{"iopub.status.busy":"2022-04-07T01:49:12.503727Z","iopub.execute_input":"2022-04-07T01:49:12.504390Z","iopub.status.idle":"2022-04-07T01:49:12.526691Z","shell.execute_reply.started":"2022-04-07T01:49:12.504335Z","shell.execute_reply":"2022-04-07T01:49:12.525542Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# Step 4: One-hot encoding\n\nUse the next code cell to one-hot encode the data in `X_train` and `X_valid`.  Set the preprocessed DataFrames to `OH_X_train` and `OH_X_valid`, respectively.  \n- The full list of categorical columns in the dataset can be found in the Python list `object_cols`.\n- You should only one-hot encode the categorical columns in `low_cardinality_cols`.  All other categorical columns should be dropped from the dataset. ","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\n# Use as many lines of code as you need!\n\noneHotEncoder = OneHotEncoder(handle_unknown = 'ignore', sparse = False)\nOH_cols_train = pd.DataFrame(oneHotEncoder.fit_transform(X_train[low_cardinality_cols])) # Your code here\nOH_cols_valid = pd.DataFrame(oneHotEncoder.transform(X_valid[low_cardinality_cols])) # Your code here\n\nOH_cols_train.index = X_train.index\nOH_cols_valid.index = X_valid.index\n\ntemp_OH_X_train = X_train.drop(low_cardinality_cols, axis = 1)\ntemp_OH_X_valid = X_valid.drop(low_cardinality_cols, axis = 1)\n\nOH_X_train = pd.concat([temp_OH_X_train, OH_cols_train], axis=1)\nOH_X_valid = pd.concat([temp_OH_X_valid, OH_cols_valid], axis=1)\n\nOH_X_train = OH_X_train.drop(high_cardinality_cols, axis = 1)\nOH_X_valid = OH_X_valid.drop(high_cardinality_cols, axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-04-07T01:49:12.528466Z","iopub.execute_input":"2022-04-07T01:49:12.528750Z","iopub.status.idle":"2022-04-07T01:49:12.571798Z","shell.execute_reply.started":"2022-04-07T01:49:12.528714Z","shell.execute_reply":"2022-04-07T01:49:12.570957Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"Run the next code cell to get the MAE for this approach.","metadata":{}},{"cell_type":"code","source":"print(\"MAE from Approach 3 (One-Hot Encoding):\") \nprint(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))","metadata":{"execution":{"iopub.status.busy":"2022-04-07T01:49:12.583687Z","iopub.execute_input":"2022-04-07T01:49:12.584036Z","iopub.status.idle":"2022-04-07T01:49:14.314284Z","shell.execute_reply.started":"2022-04-07T01:49:12.584000Z","shell.execute_reply":"2022-04-07T01:49:14.313043Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"model = RandomForestRegressor(n_estimators = 100, random_state = 0)\nmodel.fit(OH_X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-04-07T01:49:14.316044Z","iopub.execute_input":"2022-04-07T01:49:14.316322Z","iopub.status.idle":"2022-04-07T01:49:15.986440Z","shell.execute_reply.started":"2022-04-07T01:49:14.316288Z","shell.execute_reply":"2022-04-07T01:49:15.985499Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# Step 5 : Testing Data with encoded model\nWith encoded model we'll testing predict the test_x and see how accurate our data with one-hot, and some missing value preprocessing with simple imputer. We'll compare the predict test with sample_submission.csv","metadata":{}},{"cell_type":"code","source":"# One-hot encoding\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n\nOH_encoder.fit(X_train[low_cardinality_cols])\nX_cols_test = pd.DataFrame(OH_encoder.transform(X_test[low_cardinality_cols]))\n\nX_cols_test.index = X_test.index\n\nnum_X_test = X_test.drop(low_cardinality_cols, axis = 1)\n\nOH_x_test = pd.concat([X_cols_test, num_X_test], axis = 1)\n\nOH_x_test = OH_x_test.drop(high_cardinality_cols, axis = 1)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-07T01:49:15.988305Z","iopub.execute_input":"2022-04-07T01:49:15.988637Z","iopub.status.idle":"2022-04-07T01:49:16.032940Z","shell.execute_reply.started":"2022-04-07T01:49:15.988596Z","shell.execute_reply":"2022-04-07T01:49:16.032176Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# We'll using simple imputer to preprocessing our missing values\nfrom sklearn.impute import SimpleImputer\n\nmissing_columns = OH_x_test.isnull().sum() > 10\nmissing_columns = list(missing_columns [missing_columns == True].index)\n\nimputer = SimpleImputer(strategy = 'median')\n\nfinal_X_test = pd.DataFrame(imputer.fit_transform(OH_x_test))\nfinal_X_test.columns = OH_x_test.columns\n\nfinal_X_test =  final_X_test.drop(missing_columns, axis = 1)\n\npred = model.predict(final_X_test)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-07T01:49:16.034489Z","iopub.execute_input":"2022-04-07T01:49:16.034850Z","iopub.status.idle":"2022-04-07T01:49:16.093998Z","shell.execute_reply.started":"2022-04-07T01:49:16.034820Z","shell.execute_reply":"2022-04-07T01:49:16.093138Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"valid_data = pd.read_csv('../input/sample_submission.csv', index_col = 'Id')\nprint(f\"MAE from step 5 (One-Hot Encoding and Simple Imputer): {mean_absolute_error(pred, valid_data)}\") ","metadata":{"execution":{"iopub.status.busy":"2022-04-07T01:52:22.464019Z","iopub.execute_input":"2022-04-07T01:52:22.465433Z","iopub.status.idle":"2022-04-07T01:52:22.482107Z","shell.execute_reply.started":"2022-04-07T01:52:22.465365Z","shell.execute_reply":"2022-04-07T01:52:22.481178Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"output = pd.DataFrame({\n    'Id' : X_test.index,\n    'SalePrice' : pred\n})\noutput.to_csv('./submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-07T01:49:16.213206Z","iopub.status.idle":"2022-04-07T01:49:16.213995Z","shell.execute_reply.started":"2022-04-07T01:49:16.213744Z","shell.execute_reply":"2022-04-07T01:49:16.213776Z"},"trusted":true},"execution_count":null,"outputs":[]}]}